{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5765bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94414bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/functools.py:912: ImplicitModificationWarning: Transforming to str index.\n",
      "  return dispatch(args[0].__class__)(*args, **kw)\n"
     ]
    }
   ],
   "source": [
    "import anndata as ad\n",
    "import h5py\n",
    "import zarr\n",
    "from cellflow.data._utils import write_sharded\n",
    "from anndata.experimental import read_lazy\n",
    "from cellflow.data import DataManager\n",
    "import cupy as cp\n",
    "import tqdm\n",
    "import dask\n",
    "import numpy as np\n",
    "\n",
    "print(\"loading data\")\n",
    "with h5py.File(\"/lustre/groups/ml01/workspace/100mil/100m_int_indices.h5ad\", \"r\") as f:\n",
    "    adata_all = ad.AnnData(\n",
    "        obs=ad.io.read_elem(f[\"obs\"]),\n",
    "        var=read_lazy(f[\"var\"]),\n",
    "        uns = read_lazy(f[\"uns\"]),\n",
    "        obsm = read_lazy(f[\"obsm\"]),\n",
    "    )\n",
    "\n",
    "dm = DataManager(adata_all,  \n",
    "    sample_rep=\"X_pca\",\n",
    "    control_key=\"control\",\n",
    "    perturbation_covariates={\"drugs\": (\"drug\",), \"dosage\": (\"dosage\",)},\n",
    "    perturbation_covariate_reps={\"drugs\": \"drug_embeddings\"},\n",
    "    sample_covariates=[\"cell_line\"],\n",
    "    sample_covariate_reps={\"cell_line\": \"cell_line_embeddings\"},\n",
    "    split_covariates=[\"cell_line\"],\n",
    "    max_combination_length=None,\n",
    "    null_value=0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ac0f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 910.75 ms\n",
      "[########################################] | 100% Completed | 23.67 s\n",
      "[########################################] | 100% Completed | 252.54 s\n"
     ]
    }
   ],
   "source": [
    "cond_data = dm._get_condition_data(adata=adata_all)\n",
    "cell_data = dm._get_cell_data(adata_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9adbd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing source to cell data idcs: 100%|██████████| 50/50 [00:00<00:00, 62.77it/s]\n",
      "Computing target to cell data idcs: 100%|██████████| 56827/56827 [01:05<00:00, 863.54it/s]\n"
     ]
    }
   ],
   "source": [
    "n_source_dists = len(cond_data.split_idx_to_covariates)\n",
    "n_target_dists = len(cond_data.perturbation_idx_to_covariates)\n",
    "\n",
    "tgt_cell_data = {}\n",
    "src_cell_data = {}\n",
    "gpu_per_cov_mask = cp.asarray(cond_data.perturbation_covariates_mask)\n",
    "gpu_spl_cov_mask = cp.asarray(cond_data.split_covariates_mask)\n",
    "\n",
    "for src_idx in tqdm.tqdm(range(n_source_dists), desc=\"Computing source to cell data idcs\"):\n",
    "    mask = gpu_spl_cov_mask == src_idx\n",
    "    src_cell_data[str(src_idx)] = {\n",
    "        \"cell_data_index\": cp.where(mask)[0].get(),\n",
    "    }\n",
    "\n",
    "for tgt_idx in tqdm.tqdm(range(n_target_dists), desc=\"Computing target to cell data idcs\"):\n",
    "    mask = gpu_per_cov_mask == tgt_idx\n",
    "    tgt_cell_data[str(tgt_idx)] = {\n",
    "        \"cell_data_index\": cp.where(mask)[0].get(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dad2d31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing source to cell data: 100%|██████████| 50/50 [00:00<00:00, 22329.13it/s]\n",
      "Computing target to cell data:   6%|▌         | 3184/56827 [00:00<00:01, 31833.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing target to cell data: 100%|██████████| 56827/56827 [00:02<00:00, 23426.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#####################                   ] | 52% Completed | 36m 17ss"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 73m 49s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import dask.array as da\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "src_delayed_objs = []\n",
    "for src_idx in tqdm.tqdm(range(n_source_dists), desc=\"Computing source to cell data\"):\n",
    "    indices = src_cell_data[str(src_idx)][\"cell_data_index\"]\n",
    "    delayed_obj = dask.delayed(lambda x: cell_data[x])(indices)\n",
    "    src_delayed_objs.append((str(src_idx), delayed_obj))\n",
    "\n",
    "tgt_delayed_objs = []\n",
    "for tgt_idx in tqdm.tqdm(range(n_target_dists), desc=\"Computing target to cell data\"):\n",
    "    indices = tgt_cell_data[str(tgt_idx)][\"cell_data_index\"]\n",
    "    delayed_obj = dask.delayed(lambda x: cell_data[x])(indices)\n",
    "    tgt_delayed_objs.append((str(tgt_idx), delayed_obj))\n",
    "\n",
    "src_results = []\n",
    "tgt_results = []\n",
    "with ProgressBar():\n",
    "    src_results, tgt_results = dask.compute(src_delayed_objs, tgt_delayed_objs)\n",
    "\n",
    "for k, v in src_results:\n",
    "    src_cell_data[k][\"cell_data\"] = v\n",
    "\n",
    "for k, v in tgt_results:\n",
    "    tgt_cell_data[k][\"cell_data\"] = v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6c007b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split_covariates_mask = np.asarray(cond_data.split_covariates_mask)\n",
    "perturbation_covariates_mask = np.asarray(cond_data.perturbation_covariates_mask)\n",
    "condition_data = {str(k): np.asarray(v) for k, v in (cond_data.condition_data or {}).items()}\n",
    "control_to_perturbation = {str(k): np.asarray(v) for k, v in (cond_data.control_to_perturbation or {}).items()}\n",
    "split_idx_to_covariates = {str(k): np.asarray(v) for k, v in (cond_data.split_idx_to_covariates or {}).items()}\n",
    "perturbation_idx_to_covariates = {\n",
    "    str(k): np.asarray(v) for k, v in (cond_data.perturbation_idx_to_covariates or {}).items()\n",
    "}\n",
    "perturbation_idx_to_id = {str(k): v for k, v in (cond_data.perturbation_idx_to_id or {}).items()}\n",
    "\n",
    "train_data_dict = {\n",
    "    \"split_covariates_mask\": split_covariates_mask,\n",
    "    \"perturbation_covariates_mask\": perturbation_covariates_mask,\n",
    "    \"split_idx_to_covariates\": split_idx_to_covariates,\n",
    "    \"perturbation_idx_to_covariates\": perturbation_idx_to_covariates,\n",
    "    \"perturbation_idx_to_id\": perturbation_idx_to_id,\n",
    "    \"condition_data\": condition_data,\n",
    "    \"control_to_perturbation\": control_to_perturbation,\n",
    "    \"max_combination_length\": int(cond_data.max_combination_length),\n",
    "    # \"src_cell_data\": src_cell_data,\n",
    "    # \"tgt_cell_data\": tgt_cell_data,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f224240",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/lustre/groups/ml01/workspace/100mil/tahoe.zarr\"\n",
    "zgroup = zarr.open_group(path, mode=\"w\")\n",
    "chunk_size = 65536\n",
    "shard_size = chunk_size * 16\n",
    "\n",
    "ad.settings.zarr_write_format = 3  # Needed to support sharding in Zarr\n",
    "\n",
    "def get_size(shape: tuple[int, ...], chunk_size: int, shard_size: int) -> tuple[int, int]:\n",
    "    shard_size_used = shard_size\n",
    "    chunk_size_used = chunk_size\n",
    "    if chunk_size > shape[0] or shard_size > shape[0]:\n",
    "        chunk_size_used = shard_size_used = shape[0]\n",
    "    return chunk_size_used, shard_size_used\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "710434e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Allocating cell data:  14%|█▍        | 7/50 [00:45<04:47,  6.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Allocating cell data: 100%|██████████| 50/50 [06:47<00:00,  8.15s/it]\n",
      "Allocating cell data: 100%|██████████| 56827/56827 [41:54<00:00, 22.60it/s]    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def write_arr(z_arr, arr, k):\n",
    "    z_arr[:] = arr\n",
    "    return k\n",
    "\n",
    "def allocate_cell_data(group, cell_data, chunk_size, shard_size):\n",
    "    delayed_objs = []\n",
    "\n",
    "    for k in tqdm.tqdm(cell_data.keys(), desc=\"Allocating cell data\"):\n",
    "        chunk_size_used, shard_size_used = get_size(cell_data[k][\"cell_data\"].shape, chunk_size, shard_size)\n",
    "        arr = cell_data[k][\"cell_data\"]\n",
    "\n",
    "        z_arr = group.create_array(\n",
    "            name=k,\n",
    "            shape=arr.shape,\n",
    "            chunks=(chunk_size_used, arr.shape[1]),\n",
    "            shards=(shard_size_used, arr.shape[1]),\n",
    "            compressors=None,\n",
    "            dtype=arr.dtype,\n",
    "        )\n",
    "\n",
    "        delayed_objs.append(dask.delayed(write_arr)(z_arr, arr, k))\n",
    "    \n",
    "    return delayed_objs\n",
    "\n",
    "\n",
    "src_group = zgroup.create_group(\"src_cell_data\", overwrite=True)\n",
    "tgt_group = zgroup.create_group(\"tgt_cell_data\", overwrite=True)\n",
    "\n",
    "\n",
    "src_delayed_objs = allocate_cell_data(src_group, src_cell_data, chunk_size, shard_size)\n",
    "tgt_delayed_objs = allocate_cell_data(tgt_group, tgt_cell_data, chunk_size, shard_size)\n",
    "\n",
    "\n",
    "\n",
    "# for k in tqdm.tqdm(src_cell_data.keys(), desc=\"Writing src cell data\"):\n",
    "#     chunk_size_used, shard_size_used = get_size(src_cell_data[k][\"cell_data\"].shape, chunk_size, shard_size)\n",
    "#     arr = src_cell_data[k][\"cell_data\"]\n",
    "\n",
    "#     z_arr = src_group.create_array(\n",
    "#         name=k,\n",
    "#         shape=arr.shape,\n",
    "#         chunks=(chunk_size_used, arr.shape[1]),\n",
    "#         shards=(shard_size_used, arr.shape[1]),\n",
    "#         compressors=None,\n",
    "#         dtype=arr.dtype,\n",
    "#     )\n",
    "    \n",
    "#     delayed_objs.append(dask.delayed(write_arr)(z_arr, arr, k))\n",
    "\n",
    "# for k in tqdm.tqdm(tgt_cell_data.keys(), desc=\"Writing tgt cell data\"):\n",
    "#     chunk_size_used, shard_size_used = get_size(tgt_cell_data[k][\"cell_data\"].shape, chunk_size, shard_size)\n",
    "#     arr = tgt_cell_data[k][\"cell_data\"]\n",
    "#     z_arr = tgt_group.create_array(\n",
    "#         name=k,\n",
    "#         shape=arr.shape,\n",
    "#         chunks=(chunk_size_used, arr.shape[1]),\n",
    "#         shards=(shard_size_used, arr.shape[1]),\n",
    "#         compressors=None,\n",
    "#         dtype=arr.dtype,\n",
    "#     )\n",
    "    \n",
    "    \n",
    "#     delayed_objs.append(dask.delayed(write_arr)(z_arr, arr, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c41b2a3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ProgressBar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mProgressBar\u001b[49m():\n\u001b[32m      2\u001b[39m     res = dask.compute(tgt_delayed_objs)\n",
      "\u001b[31mNameError\u001b[39m: name 'ProgressBar' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "with ProgressBar():\n",
    "    res = dask.compute(tgt_delayed_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28f54507",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_group() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m mapping_data = \u001b[43mzarr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmapping_data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m write_sharded(\n\u001b[32m      4\u001b[39m     mapping_data,\n\u001b[32m      5\u001b[39m     train_data_dict,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     compressors=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdone\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: create_group() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "mapping_data = zarr.create_group(zgroup, \"mapping_data\")\n",
    "\n",
    "write_sharded(\n",
    "    mapping_data,\n",
    "    train_data_dict,\n",
    "    chunk_size=chunk_size,\n",
    "    shard_size=shard_size,\n",
    "    compressors=None,\n",
    ")\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
