{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5765bb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/functools.py:912: ImplicitModificationWarning: Transforming to str index.\n",
      "  return dispatch(args[0].__class__)(*args, **kw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "[########################################] | 100% Completed | 1.11 sms\n",
      "[########################################] | 100% Completed | 25.93 s\n",
      "[########################################] | 100% Completed | 294.61 s\n"
     ]
    }
   ],
   "source": [
    "%%\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# %%\n",
    "import anndata as ad\n",
    "import h5py\n",
    "import zarr\n",
    "from cellflow.data._utils import write_sharded\n",
    "from anndata.experimental import read_lazy\n",
    "from cellflow.data import DataManager\n",
    "import cupy as cp\n",
    "import tqdm\n",
    "import dask\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "from dask.diagnostics import ProgressBar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd4946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing source to cell data idcs: 100%|██████████| 50/50 [00:00<00:00, 87.79it/s]\n",
      "Computing target to cell data idcs:  68%|██████▊   | 38602/56827 [00:45<00:21, 854.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing target to cell data idcs: 100%|██████████| 56827/56827 [01:06<00:00, 852.83it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "35894e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_data = cell_data.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310df180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 106.87 GiB </td>\n",
       "                        <td> 1.14 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (95624334, 300) </td>\n",
       "                        <td> (1000, 300) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 95625 chunks in 3 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float32 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"6\" x2=\"25\" y2=\"6\" />\n",
       "  <line x1=\"0\" y1=\"12\" x2=\"25\" y2=\"12\" />\n",
       "  <line x1=\"0\" y1=\"18\" x2=\"25\" y2=\"18\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"25\" y2=\"25\" />\n",
       "  <line x1=\"0\" y1=\"31\" x2=\"25\" y2=\"31\" />\n",
       "  <line x1=\"0\" y1=\"37\" x2=\"25\" y2=\"37\" />\n",
       "  <line x1=\"0\" y1=\"44\" x2=\"25\" y2=\"44\" />\n",
       "  <line x1=\"0\" y1=\"50\" x2=\"25\" y2=\"50\" />\n",
       "  <line x1=\"0\" y1=\"56\" x2=\"25\" y2=\"56\" />\n",
       "  <line x1=\"0\" y1=\"63\" x2=\"25\" y2=\"63\" />\n",
       "  <line x1=\"0\" y1=\"69\" x2=\"25\" y2=\"69\" />\n",
       "  <line x1=\"0\" y1=\"75\" x2=\"25\" y2=\"75\" />\n",
       "  <line x1=\"0\" y1=\"82\" x2=\"25\" y2=\"82\" />\n",
       "  <line x1=\"0\" y1=\"88\" x2=\"25\" y2=\"88\" />\n",
       "  <line x1=\"0\" y1=\"94\" x2=\"25\" y2=\"94\" />\n",
       "  <line x1=\"0\" y1=\"101\" x2=\"25\" y2=\"101\" />\n",
       "  <line x1=\"0\" y1=\"107\" x2=\"25\" y2=\"107\" />\n",
       "  <line x1=\"0\" y1=\"113\" x2=\"25\" y2=\"113\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,120.0 0.0,120.0\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >300</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">95624334</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<asnumpy, shape=(95624334, 300), dtype=float32, chunksize=(1000, 300), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303064c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 300)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.take(cell_data, , axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9a9e4693",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_data_batch = cell_data[:100_000].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8b45658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spl_cov_mask_batch = gpu_spl_cov_mask[:100_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ee2e0760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "       16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,\n",
       "       33, 34, 35, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.unique(spl_cov_mask_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "55ded52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = (gpu_per_cov_mask-gpu_spl_cov_mask+50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "99ee8158",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = cp.argsort(mapping)\n",
    "ordered_mapping = mapping[sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "615b59c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values, inverse_indices = cp.unique(ordered_mapping, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cd48c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_cell_data = da.take(cell_data,sorted_indices.get(),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ffe82904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 106.87 GiB </td>\n",
       "                        <td> 1.21 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (95624334, 300) </td>\n",
       "                        <td> (1053, 300) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 95720 chunks in 4 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float32 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"6\" x2=\"25\" y2=\"6\" />\n",
       "  <line x1=\"0\" y1=\"12\" x2=\"25\" y2=\"12\" />\n",
       "  <line x1=\"0\" y1=\"18\" x2=\"25\" y2=\"18\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"25\" y2=\"25\" />\n",
       "  <line x1=\"0\" y1=\"31\" x2=\"25\" y2=\"31\" />\n",
       "  <line x1=\"0\" y1=\"37\" x2=\"25\" y2=\"37\" />\n",
       "  <line x1=\"0\" y1=\"44\" x2=\"25\" y2=\"44\" />\n",
       "  <line x1=\"0\" y1=\"50\" x2=\"25\" y2=\"50\" />\n",
       "  <line x1=\"0\" y1=\"56\" x2=\"25\" y2=\"56\" />\n",
       "  <line x1=\"0\" y1=\"63\" x2=\"25\" y2=\"63\" />\n",
       "  <line x1=\"0\" y1=\"69\" x2=\"25\" y2=\"69\" />\n",
       "  <line x1=\"0\" y1=\"75\" x2=\"25\" y2=\"75\" />\n",
       "  <line x1=\"0\" y1=\"82\" x2=\"25\" y2=\"82\" />\n",
       "  <line x1=\"0\" y1=\"88\" x2=\"25\" y2=\"88\" />\n",
       "  <line x1=\"0\" y1=\"94\" x2=\"25\" y2=\"94\" />\n",
       "  <line x1=\"0\" y1=\"101\" x2=\"25\" y2=\"101\" />\n",
       "  <line x1=\"0\" y1=\"107\" x2=\"25\" y2=\"107\" />\n",
       "  <line x1=\"0\" y1=\"113\" x2=\"25\" y2=\"113\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,120.0 0.0,120.0\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >300</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">95624334</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<getitem, shape=(95624334, 300), dtype=float32, chunksize=(1053, 300), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord_cell_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a636955a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ord_cell_data = \u001b[43mord_cell_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/base.py:373\u001b[39m, in \u001b[36mDaskMethodsMixin.compute\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m    350\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[32m    351\u001b[39m \n\u001b[32m    352\u001b[39m \u001b[33;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    371\u001b[39m \u001b[33;03m    dask.compute\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m     (result,) = \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/base.py:681\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     expr = expr.optimize()\n\u001b[32m    679\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(flatten(expr.__dask_keys__()))\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/_task_spec.py:272\u001b[39m, in \u001b[36mconvert_legacy_graph\u001b[39m\u001b[34m(dsk, all_keys)\u001b[39m\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, GraphNode):\n\u001b[32m    271\u001b[39m         t = DataNode(k, t)\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     new_dsk[k] = t\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_dsk\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "ord_cell_data = ord_cell_data.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48f9c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ..., 32, 31, 38], shape=(95624334,), dtype=int32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_spl_cov_mask[:100_000].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fcc7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in list(src_cell_data.keys()):\n",
    "    idx = src_cell_data[k][\"cell_data_index\"]\n",
    "    src_cell_data[k][\"cell_data\"] = da.take(cell_data, idx, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df755b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fda2c9f5100>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/icb/selman.ozleyen/.local/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 790, in _clean_thread_parent_frames\n",
      "    active_threads = {thread.ident for thread in threading.enumerate()}\n",
      "                      ^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(tgt_cell_data.keys()):\n\u001b[32m      2\u001b[39m     idx = tgt_cell_data[k][\u001b[33m\"\u001b[39m\u001b[33mcell_data_index\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     tgt_cell_data[k][\u001b[33m\"\u001b[39m\u001b[33mcell_data\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/array/routines.py:2013\u001b[39m, in \u001b[36mtake\u001b[39m\u001b[34m(a, indices, axis)\u001b[39m\n\u001b[32m   2011\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _take_dask_array_from_numpy(a, indices, axis)\n\u001b[32m   2012\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2013\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/array/core.py:2038\u001b[39m, in \u001b[36mArray.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m   2036\u001b[39m out = \u001b[33m\"\u001b[39m\u001b[33mgetitem-\u001b[39m\u001b[33m\"\u001b[39m + tokenize(\u001b[38;5;28mself\u001b[39m, index2)\n\u001b[32m   2037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2038\u001b[39m     dsk, chunks = \u001b[43mslice_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2039\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SlicingNoop:\n\u001b[32m   2040\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args, **kwds):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/array/slicing.py:172\u001b[39m, in \u001b[36mslice_array\u001b[39m\u001b[34m(out_name, in_name, blockdims, index)\u001b[39m\n\u001b[32m    169\u001b[39m index += (\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m),) * missing\n\u001b[32m    171\u001b[39m \u001b[38;5;66;03m# Pass down to next function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m dsk_out, bd_out = \u001b[43mslice_with_newaxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m bd_out = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m, bd_out))\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dsk_out, bd_out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/array/slicing.py:194\u001b[39m, in \u001b[36mslice_with_newaxes\u001b[39m\u001b[34m(out_name, in_name, blockdims, index)\u001b[39m\n\u001b[32m    191\u001b[39m         where_none[i] -= n\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Pass down and do work\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m dsk, blockdims2 = \u001b[43mslice_wrap_lists\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere_none\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where_none:\n\u001b[32m    199\u001b[39m     expand = expander(where_none)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/array/slicing.py:271\u001b[39m, in \u001b[36mslice_wrap_lists\u001b[39m\u001b[34m(out_name, in_name, blockdims, index, allow_getitem_optimization)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(is_arraylike(i) \u001b[38;5;129;01mor\u001b[39;00m i == \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m index):\n\u001b[32m    270\u001b[39m     axis = where_list[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     blockdims2, dsk3 = \u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwhere_list\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;66;03m# Mixed case. Both slices/integers and lists. slice/integer then take\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    276\u001b[39m     \u001b[38;5;66;03m# Do first pass without lists\u001b[39;00m\n\u001b[32m    277\u001b[39m     tmp = \u001b[33m\"\u001b[39m\u001b[33mslice-\u001b[39m\u001b[33m\"\u001b[39m + tokenize((out_name, in_name, blockdims, index))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/array/slicing.py:630\u001b[39m, in \u001b[36mtake\u001b[39m\u001b[34m(outname, inname, chunks, index, axis)\u001b[39m\n\u001b[32m    623\u001b[39m         indexer.append(index[i : i + average_chunk_size].tolist())\n\u001b[32m    625\u001b[39m     token = (\n\u001b[32m    626\u001b[39m         outname.split(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m]\n\u001b[32m    627\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m outname\n\u001b[32m    628\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m tokenize(outname, chunks, index, axis)\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m     chunks, graph = \u001b[43m_shuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks, graph\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunks[axis]) == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/array/_shuffle.py:212\u001b[39m, in \u001b[36m_shuffle\u001b[39m\u001b[34m(chunks, indexer, axis, in_name, out_name, token)\u001b[39m\n\u001b[32m    209\u001b[39m     new_chunks.append(current_chunk)\n\u001b[32m    211\u001b[39m \u001b[38;5;66;03m# force 64 bit to avoid potential integer overflows on win32 and numpy<2\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m chunk_boundaries = np.cumsum(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m[\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muint64\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    214\u001b[39m \u001b[38;5;66;03m# Get existing chunk tuple locations\u001b[39;00m\n\u001b[32m    215\u001b[39m chunk_tuples = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m    216\u001b[39m     product(*(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(c)) \u001b[38;5;28;01mfor\u001b[39;00m i, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks) \u001b[38;5;28;01mif\u001b[39;00m i != axis))\n\u001b[32m    217\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(tgt_cell_data.keys()):\n\u001b[32m      2\u001b[39m     idx = tgt_cell_data[k][\u001b[33m\"\u001b[39m\u001b[33mcell_data_index\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     tgt_cell_data[k][\u001b[33m\"\u001b[39m\u001b[33mcell_data\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/array/routines.py:2013\u001b[39m, in \u001b[36mtake\u001b[39m\u001b[34m(a, indices, axis)\u001b[39m\n\u001b[32m   2011\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _take_dask_array_from_numpy(a, indices, axis)\n\u001b[32m   2012\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2013\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/array/core.py:2038\u001b[39m, in \u001b[36mArray.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m   2036\u001b[39m out = \u001b[33m\"\u001b[39m\u001b[33mgetitem-\u001b[39m\u001b[33m\"\u001b[39m + tokenize(\u001b[38;5;28mself\u001b[39m, index2)\n\u001b[32m   2037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2038\u001b[39m     dsk, chunks = \u001b[43mslice_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2039\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SlicingNoop:\n\u001b[32m   2040\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args, **kwds):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/array/slicing.py:172\u001b[39m, in \u001b[36mslice_array\u001b[39m\u001b[34m(out_name, in_name, blockdims, index)\u001b[39m\n\u001b[32m    169\u001b[39m index += (\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m),) * missing\n\u001b[32m    171\u001b[39m \u001b[38;5;66;03m# Pass down to next function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m dsk_out, bd_out = \u001b[43mslice_with_newaxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m bd_out = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m, bd_out))\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dsk_out, bd_out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/array/slicing.py:194\u001b[39m, in \u001b[36mslice_with_newaxes\u001b[39m\u001b[34m(out_name, in_name, blockdims, index)\u001b[39m\n\u001b[32m    191\u001b[39m         where_none[i] -= n\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Pass down and do work\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m dsk, blockdims2 = \u001b[43mslice_wrap_lists\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere_none\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where_none:\n\u001b[32m    199\u001b[39m     expand = expander(where_none)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/array/slicing.py:271\u001b[39m, in \u001b[36mslice_wrap_lists\u001b[39m\u001b[34m(out_name, in_name, blockdims, index, allow_getitem_optimization)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(is_arraylike(i) \u001b[38;5;129;01mor\u001b[39;00m i == \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m index):\n\u001b[32m    270\u001b[39m     axis = where_list[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     blockdims2, dsk3 = \u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwhere_list\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;66;03m# Mixed case. Both slices/integers and lists. slice/integer then take\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    276\u001b[39m     \u001b[38;5;66;03m# Do first pass without lists\u001b[39;00m\n\u001b[32m    277\u001b[39m     tmp = \u001b[33m\"\u001b[39m\u001b[33mslice-\u001b[39m\u001b[33m\"\u001b[39m + tokenize((out_name, in_name, blockdims, index))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/array/slicing.py:630\u001b[39m, in \u001b[36mtake\u001b[39m\u001b[34m(outname, inname, chunks, index, axis)\u001b[39m\n\u001b[32m    623\u001b[39m         indexer.append(index[i : i + average_chunk_size].tolist())\n\u001b[32m    625\u001b[39m     token = (\n\u001b[32m    626\u001b[39m         outname.split(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m]\n\u001b[32m    627\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m outname\n\u001b[32m    628\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m tokenize(outname, chunks, index, axis)\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m     chunks, graph = \u001b[43m_shuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks, graph\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunks[axis]) == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/array/_shuffle.py:229\u001b[39m, in \u001b[36m_shuffle\u001b[39m\u001b[34m(chunks, indexer, axis, in_name, out_name, token)\u001b[39m\n\u001b[32m    225\u001b[39m sorter_name = \u001b[33m\"\u001b[39m\u001b[33mshuffle-sorter-\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    226\u001b[39m taker_name = \u001b[33m\"\u001b[39m\u001b[33mshuffle-taker-\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    228\u001b[39m old_blocks = {\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     old_index: (in_name,) + old_index\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m old_index \u001b[38;5;129;01min\u001b[39;00m np.ndindex(\u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;28mlen\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m chunks]))\n\u001b[32m    231\u001b[39m }\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m new_chunk_idx, new_chunk_taker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(new_chunks):\n\u001b[32m    233\u001b[39m     new_chunk_taker = np.array(new_chunk_taker)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for k in list(tgt_cell_data.keys()):\n",
    "    idx = tgt_cell_data[k][\"cell_data_index\"]\n",
    "    tgt_cell_data[k][\"cell_data\"] = da.take(cell_data, idx, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2e3a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Availible mem 34.36\n",
      "Using batch size: 2,300,000 rows\n",
      "Estimated memory per batch: 5.52 GB\n"
     ]
    }
   ],
   "source": [
    "mempool = cp.get_default_memory_pool()\n",
    "mempool.set_limit(40 * 1024**3)  # Set limit to 40 GB\n",
    "batch_size = 2_300_000\n",
    "gpu_fraction = 0.8\n",
    "available_memory = mempool.get_limit() * gpu_fraction\n",
    "\n",
    "# Calculate optimal batch size based on memory\n",
    "bytes_per_element = cell_data.dtype.itemsize\n",
    "elements_per_row = cell_data.shape[1]\n",
    "bytes_per_row = bytes_per_element * elements_per_row\n",
    "\n",
    "# Reserve memory for both input and output\n",
    "max_batch_size = int(available_memory / (bytes_per_row * 2))\n",
    "actual_batch_size = min(batch_size, max_batch_size)\n",
    "\n",
    "print(f\"Using batch size: {actual_batch_size:,} rows\")\n",
    "print(f\"Estimated memory per batch: {(actual_batch_size * bytes_per_row * 2) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b36da19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_indices_gpu(indices_dict: Dict, description: str) -> Dict:\n",
    "    \"\"\"Process a dictionary of indices on GPU\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for key in tqdm.tqdm(indices_dict.keys(), desc=description):\n",
    "        indices = indices_dict[key][\"cell_data_index\"]\n",
    "        \n",
    "        if len(indices) == 0:\n",
    "            results[key] = {\"cell_data\": np.empty((0, cell_data.shape[1]), dtype=cell_data.dtype)}\n",
    "            continue\n",
    "        \n",
    "        # Process in batches if indices are large\n",
    "        if len(indices) <= actual_batch_size:\n",
    "            # Small enough to process at once\n",
    "            gpu_result = process_single_batch_gpu(cell_data, indices)\n",
    "            results[key] = {\"cell_data\": gpu_result}\n",
    "        else:\n",
    "            # Process in multiple batches\n",
    "            batched_results = []\n",
    "            n_batches = (len(indices) + actual_batch_size - 1) // actual_batch_size\n",
    "            \n",
    "            for batch_idx in range(n_batches):\n",
    "                start_idx = batch_idx * actual_batch_size\n",
    "                end_idx = min((batch_idx + 1) * actual_batch_size, len(indices))\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "                \n",
    "                batch_result = process_single_batch_gpu(cell_data, batch_indices)\n",
    "                batched_results.append(batch_result)\n",
    "                \n",
    "                # Clear GPU memory between batches\n",
    "                cp.get_default_memory_pool().free_all_blocks()\n",
    "            \n",
    "            # Concatenate results\n",
    "            final_result = np.concatenate(batched_results, axis=0)\n",
    "            results[key] = {\"cell_data\": final_result}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67262e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "import gc\n",
    "\n",
    "def process_cell_data_gpu_batched(\n",
    "    cell_data,\n",
    "    src_cell_data: Dict,\n",
    "    tgt_cell_data: Dict,\n",
    "    batch_size: int = 20000,  # Adjust based on GPU memory\n",
    "    gpu_memory_fraction: float = 0.8\n",
    ") -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Process cell data indexing on GPU in batches to manage memory efficiently.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cell_data : dask.array or numpy array\n",
    "        The main cell data array\n",
    "    src_cell_data : dict\n",
    "        Dictionary containing source cell data indices\n",
    "    tgt_cell_data : dict\n",
    "        Dictionary containing target cell data indices\n",
    "    batch_size : int\n",
    "        Number of rows to process per batch\n",
    "    gpu_memory_fraction : float\n",
    "        Fraction of GPU memory to use for cell data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of updated src_cell_data and tgt_cell_data dictionaries\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get available GPU memory\n",
    "    mempool = cp.get_default_memory_pool()\n",
    "    available_memory = mempool.get_limit() * gpu_memory_fraction\n",
    "    \n",
    "    # Calculate optimal batch size based on memory\n",
    "    bytes_per_element = cell_data.dtype.itemsize\n",
    "    elements_per_row = cell_data.shape[1]\n",
    "    bytes_per_row = bytes_per_element * elements_per_row\n",
    "    \n",
    "    # Reserve memory for both input and output\n",
    "    max_batch_size = int(available_memory / (bytes_per_row * 2))\n",
    "    actual_batch_size = min(batch_size, max_batch_size)\n",
    "    \n",
    "    \n",
    "    def process_indices_gpu(indices_dict: Dict, description: str) -> Dict:\n",
    "        \"\"\"Process a dictionary of indices on GPU\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for key in tqdm.tqdm(indices_dict.keys(), desc=description):\n",
    "            indices = indices_dict[key][\"cell_data_index\"]\n",
    "            \n",
    "            if len(indices) == 0:\n",
    "                results[key] = {\"cell_data\": np.empty((0, cell_data.shape[1]), dtype=cell_data.dtype)}\n",
    "                continue\n",
    "            \n",
    "            # Process in batches if indices are large\n",
    "            if len(indices) <= actual_batch_size:\n",
    "                # Small enough to process at once\n",
    "                gpu_result = process_single_batch_gpu(cell_data, indices)\n",
    "                results[key] = {\"cell_data\": gpu_result}\n",
    "            else:\n",
    "                # Process in multiple batches\n",
    "                batched_results = []\n",
    "                n_batches = (len(indices) + actual_batch_size - 1) // actual_batch_size\n",
    "                \n",
    "                for batch_idx in range(n_batches):\n",
    "                    start_idx = batch_idx * actual_batch_size\n",
    "                    end_idx = min((batch_idx + 1) * actual_batch_size, len(indices))\n",
    "                    batch_indices = indices[start_idx:end_idx]\n",
    "                    \n",
    "                    batch_result = process_single_batch_gpu(cell_data, batch_indices)\n",
    "                    batched_results.append(batch_result)\n",
    "                    \n",
    "                    # Clear GPU memory between batches\n",
    "                    cp.get_default_memory_pool().free_all_blocks()\n",
    "                \n",
    "                # Concatenate results\n",
    "                final_result = np.concatenate(batched_results, axis=0)\n",
    "                results[key] = {\"cell_data\": final_result}\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def process_single_batch_gpu(data, indices):\n",
    "        \"\"\"Process a single batch of indices on GPU\"\"\"\n",
    "        # Move indices to GPU\n",
    "        gpu_indices = cp.asarray(indices)\n",
    "        \n",
    "        # Move data batch to GPU (only the needed rows)\n",
    "        if hasattr(data, 'compute'):  # Dask array\n",
    "            # For dask arrays, compute only the needed slices\n",
    "            cpu_batch = data[indices].compute()\n",
    "        else:  # Regular numpy array\n",
    "            cpu_batch = data[indices]\n",
    "        \n",
    "        # Move to GPU and back to CPU\n",
    "        gpu_batch = cp.asarray(cpu_batch)\n",
    "        result = cp.asnumpy(gpu_batch)\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        del gpu_batch, gpu_indices\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # Process source and target data\n",
    "    print(\"Processing source cell data on GPU...\")\n",
    "    src_results = process_indices_gpu(src_cell_data, \"Processing source indices on GPU\")\n",
    "    \n",
    "    print(\"Processing target cell data on GPU...\")\n",
    "    tgt_results = process_indices_gpu(tgt_cell_data, \"Processing target indices on GPU\")\n",
    "    \n",
    "    # Update original dictionaries\n",
    "    for key in src_results:\n",
    "        src_cell_data[key].update(src_results[key])\n",
    "    \n",
    "    for key in tgt_results:\n",
    "        tgt_cell_data[key].update(tgt_results[key])\n",
    "    \n",
    "    # Final memory cleanup\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    gc.collect()\n",
    "    \n",
    "    return src_cell_data, tgt_cell_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6d1848e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in list(src_cell_data.keys()):\n",
    "    idx = src_cell_data[k][\"cell_data_index\"]\n",
    "    src_cell_data[k][\"cell_data\"] = cell_data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dba951a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in list(tgt_cell_data.keys()):\n",
    "    idx = tgt_cell_data[k][\"cell_data_index\"]\n",
    "    tgt_cell_data[k][\"cell_data\"] = cell_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "010bd308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing /src_cell_data: 100%|██████████| 50/50 [00:03<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done writing src_cell_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing /tgt_cell_data: 100%|██████████| 56827/56827 [22:05<00:00, 42.86it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done writing tgt_cell_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"/lustre/groups/ml01/workspace/100mil/tahoe2.zarr\"\n",
    "zgroup = zarr.open_group(path, mode=\"w\")\n",
    "chunk_size = 131072\n",
    "shard_size = chunk_size * 8\n",
    "\n",
    "ad.settings.zarr_write_format = 3  # Needed to support sharding in Zarr\n",
    "\n",
    "def get_size(shape: tuple[int, ...], chunk_size: int, shard_size: int) -> tuple[int, int]:\n",
    "    shard_size_used = shard_size\n",
    "    chunk_size_used = chunk_size\n",
    "    if chunk_size > shape[0]:\n",
    "        chunk_size_used = shard_size_used = shape[0]\n",
    "    elif chunk_size < shape[0] or shard_size > shape[0]:\n",
    "        chunk_size_used = shard_size_used = shape[0]\n",
    "    return chunk_size_used, shard_size_used\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_single_array(group, key, arr, chunk_size, shard_size):\n",
    "    \"\"\"Write a single array - designed for threading\"\"\"\n",
    "    chunk_size_used, shard_size_used = get_size(arr.shape, chunk_size, shard_size)\n",
    "    \n",
    "    group.create_array(\n",
    "        name=key,\n",
    "        data=arr,\n",
    "        chunks=(chunk_size_used, arr.shape[1]),\n",
    "        shards=(shard_size_used, arr.shape[1]),\n",
    "        compressors=None,\n",
    "    )\n",
    "    return key\n",
    "\n",
    "def write_cell_data_threaded(group, cell_data, chunk_size, shard_size, max_workers=8):\n",
    "    \"\"\"Write cell data using threading for I/O parallelism\"\"\"\n",
    "    \n",
    "    write_func = partial(write_single_array, group, chunk_size=chunk_size, shard_size=shard_size)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all write tasks\n",
    "        future_to_key = {\n",
    "            executor.submit(write_single_array, group, k, cell_data[k][\"cell_data\"], chunk_size, shard_size): k \n",
    "            for k in cell_data.keys()\n",
    "        }\n",
    "        \n",
    "        # Process results with progress bar\n",
    "        for future in tqdm.tqdm(\n",
    "            concurrent.futures.as_completed(future_to_key), \n",
    "            total=len(future_to_key),\n",
    "            desc=f\"Writing {group.name}\"\n",
    "        ):\n",
    "            key = future_to_key[future]\n",
    "            try:\n",
    "                future.result()  # This will raise any exceptions\n",
    "            except Exception as exc:\n",
    "                print(f'Array {key} generated an exception: {exc}')\n",
    "                raise\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "src_group = zgroup.create_group(\"src_cell_data\", overwrite=True)\n",
    "tgt_group = zgroup.create_group(\"tgt_cell_data\", overwrite=True)\n",
    "\n",
    "\n",
    "# Use the fast threaded approach you already implemented\n",
    "write_cell_data_threaded(src_group, src_cell_data, chunk_size, shard_size, max_workers=14)\n",
    "print(\"done writing src_cell_data\")\n",
    "write_cell_data_threaded(tgt_group, tgt_cell_data, chunk_size, shard_size, max_workers=14)\n",
    "print(\"done writing tgt_cell_data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd842ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3bc2cc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing source to cell data: 100%|██████████| 50/50 [00:00<00:00, 11695.68it/s]\n",
      "Computing target to cell data: 100%|██████████| 56827/56827 [00:02<00:00, 27235.88it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m tgt_results = []\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ProgressBar():\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     src_results, tgt_results = \u001b[43mdask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_delayed_objs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_delayed_objs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m src_results:\n\u001b[32m     19\u001b[39m     src_cell_data[k][\u001b[33m\"\u001b[39m\u001b[33mcell_data\u001b[39m\u001b[33m\"\u001b[39m] = v\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/base.py:681\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     expr = expr.optimize()\n\u001b[32m    679\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(flatten(expr.__dask_keys__()))\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/_expr.py:1156\u001b[39m, in \u001b[36m_HLGExprSequence.__dask_graph__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1153\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__dask_graph__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1154\u001b[39m     \u001b[38;5;66;03m# This class has to override this and not just _layer to ensure the HLGs\u001b[39;00m\n\u001b[32m   1155\u001b[39m     \u001b[38;5;66;03m# are not optimized individually\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ensure_dict(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimized_dsk\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/functools.py:998\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, instance, owner)\u001b[39m\n\u001b[32m    996\u001b[39m val = cache.get(\u001b[38;5;28mself\u001b[39m.attrname, _NOT_FOUND)\n\u001b[32m    997\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[32m--> \u001b[39m\u001b[32m998\u001b[39m     val = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    999\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1000\u001b[39m         cache[\u001b[38;5;28mself\u001b[39m.attrname] = val\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/_expr.py:1148\u001b[39m, in \u001b[36m_HLGExprSequence._optimized_dsk\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1146\u001b[39m     dsk = hlgexpr.hlg\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (optimizer := hlgexpr.low_level_optimizer) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1148\u001b[39m         dsk = \u001b[43moptimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1149\u001b[39m     graphs.append(dsk)\n\u001b[32m   1151\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m HighLevelGraph.merge(*graphs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/delayed.py:669\u001b[39m, in \u001b[36moptimize\u001b[39m\u001b[34m(dsk, keys, **kwargs)\u001b[39m\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dsk, HighLevelGraph):\n\u001b[32m    668\u001b[39m     dsk = HighLevelGraph.from_collections(\u001b[38;5;28mid\u001b[39m(dsk), dsk, dependencies=())\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m dsk = \u001b[43mdsk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcull\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dsk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/highlevelgraph.py:769\u001b[39m, in \u001b[36mHighLevelGraph.cull\u001b[39m\u001b[34m(self, keys)\u001b[39m\n\u001b[32m    767\u001b[39m layer = \u001b[38;5;28mself\u001b[39m.layers[layer_name]\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keys_set:\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m     culled_layer, culled_deps = \u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcull\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_ext_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    770\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m culled_deps:\n\u001b[32m    771\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/highlevelgraph.py:179\u001b[39m, in \u001b[36mLayer.cull\u001b[39m\u001b[34m(self, keys, all_hlg_keys)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_task_spec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cull\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     out = \u001b[43mcull\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m MaterializedLayer(out, annotations=\u001b[38;5;28mself\u001b[39m.annotations), {\n\u001b[32m    181\u001b[39m         k: \u001b[38;5;28mset\u001b[39m(v.dependencies) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m out.items()\n\u001b[32m    182\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/dask/_task_spec.py:1179\u001b[39m, in \u001b[36mcull\u001b[39m\u001b[34m(dsk, keys)\u001b[39m\n\u001b[32m   1177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(keys) == \u001b[38;5;28mlen\u001b[39m(dsk):\n\u001b[32m   1178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dsk\n\u001b[32m-> \u001b[39m\u001b[32m1179\u001b[39m work = \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1180\u001b[39m seen: \u001b[38;5;28mset\u001b[39m[KeyType] = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m   1181\u001b[39m dsk2 = {}\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "src_delayed_objs = []\n",
    "for src_idx in tqdm.tqdm(range(n_source_dists), desc=\"Computing source to cell data\"):\n",
    "    indices = src_cell_data[str(src_idx)][\"cell_data_index\"]\n",
    "    delayed_obj = dask.delayed(lambda x: cell_data[x])(indices)\n",
    "    src_delayed_objs.append((str(src_idx), delayed_obj))\n",
    "\n",
    "tgt_delayed_objs = []\n",
    "for tgt_idx in tqdm.tqdm(range(n_target_dists), desc=\"Computing target to cell data\"):\n",
    "    indices = tgt_cell_data[str(tgt_idx)][\"cell_data_index\"]\n",
    "    delayed_obj = dask.delayed(lambda x: cell_data[x])(indices)\n",
    "    tgt_delayed_objs.append((str(tgt_idx), delayed_obj))\n",
    "\n",
    "src_results = []\n",
    "tgt_results = []\n",
    "with ProgressBar():\n",
    "    src_results, tgt_results = dask.compute(src_delayed_objs, tgt_delayed_objs)\n",
    "\n",
    "for k, v in src_results:\n",
    "    src_cell_data[k][\"cell_data\"] = v\n",
    "\n",
    "for k, v in tgt_results:\n",
    "    tgt_cell_data[k][\"cell_data\"] = v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721747c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_results = []\n",
    "tgt_results = []\n",
    "with ProgressBar():\n",
    "    src_results, tgt_results = dask.compute(src_delayed_objs, tgt_delayed_objs)\n",
    "\n",
    "for k, v in src_results:\n",
    "    src_cell_data[k][\"cell_data\"] = v\n",
    "\n",
    "for k, v in tgt_results:\n",
    "    tgt_cell_data[k][\"cell_data\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ad908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "split_covariates_mask = np.asarray(cond_data.split_covariates_mask)\n",
    "perturbation_covariates_mask = np.asarray(cond_data.perturbation_covariates_mask)\n",
    "condition_data = {str(k): np.asarray(v) for k, v in (cond_data.condition_data or {}).items()}\n",
    "control_to_perturbation = {str(k): np.asarray(v) for k, v in (cond_data.control_to_perturbation or {}).items()}\n",
    "split_idx_to_covariates = {str(k): np.asarray(v) for k, v in (cond_data.split_idx_to_covariates or {}).items()}\n",
    "perturbation_idx_to_covariates = {\n",
    "    str(k): np.asarray(v) for k, v in (cond_data.perturbation_idx_to_covariates or {}).items()\n",
    "}\n",
    "perturbation_idx_to_id = {str(k): v for k, v in (cond_data.perturbation_idx_to_id or {}).items()}\n",
    "\n",
    "train_data_dict = {\n",
    "    \"split_covariates_mask\": split_covariates_mask,\n",
    "    \"perturbation_covariates_mask\": perturbation_covariates_mask,\n",
    "    \"split_idx_to_covariates\": split_idx_to_covariates,\n",
    "    \"perturbation_idx_to_covariates\": perturbation_idx_to_covariates,\n",
    "    \"perturbation_idx_to_id\": perturbation_idx_to_id,\n",
    "    \"condition_data\": condition_data,\n",
    "    \"control_to_perturbation\": control_to_perturbation,\n",
    "    \"max_combination_length\": int(cond_data.max_combination_length),\n",
    "    # \"src_cell_data\": src_cell_data,\n",
    "    # \"tgt_cell_data\": tgt_cell_data,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402c899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"prepared train_data_dict\")\n",
    "# %%\n",
    "path = \"/lustre/groups/ml01/workspace/100mil/tahoe2.zarr\"\n",
    "zgroup = zarr.open_group(path, mode=\"w\")\n",
    "chunk_size = 131072\n",
    "shard_size = chunk_size * 8\n",
    "\n",
    "ad.settings.zarr_write_format = 3  # Needed to support sharding in Zarr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff428dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_size(shape: tuple[int, ...], chunk_size: int, shard_size: int) -> tuple[int, int]:\n",
    "    shard_size_used = shard_size\n",
    "    chunk_size_used = chunk_size\n",
    "    if chunk_size > shape[0]:\n",
    "        chunk_size_used = shard_size_used = shape[0]\n",
    "    elif chunk_size < shape[0] or shard_size > shape[0]:\n",
    "        chunk_size_used = shard_size_used = shape[0]\n",
    "    return chunk_size_used, shard_size_used\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_single_array(group, key, arr, chunk_size, shard_size):\n",
    "    \"\"\"Write a single array - designed for threading\"\"\"\n",
    "    chunk_size_used, shard_size_used = get_size(arr.shape, chunk_size, shard_size)\n",
    "    \n",
    "    group.create_array(\n",
    "        name=key,\n",
    "        data=arr,\n",
    "        chunks=(chunk_size_used, arr.shape[1]),\n",
    "        shards=(shard_size_used, arr.shape[1]),\n",
    "        compressors=None,\n",
    "        dtype=arr.dtype,\n",
    "    )\n",
    "    return key\n",
    "\n",
    "def write_cell_data_threaded(group, cell_data, chunk_size, shard_size, max_workers=8):\n",
    "    \"\"\"Write cell data using threading for I/O parallelism\"\"\"\n",
    "    \n",
    "    write_func = partial(write_single_array, group, chunk_size=chunk_size, shard_size=shard_size)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all write tasks\n",
    "        future_to_key = {\n",
    "            executor.submit(write_single_array, group, k, cell_data[k][\"cell_data\"], chunk_size, shard_size): k \n",
    "            for k in cell_data.keys()\n",
    "        }\n",
    "        \n",
    "        # Process results with progress bar\n",
    "        for future in tqdm.tqdm(\n",
    "            concurrent.futures.as_completed(future_to_key), \n",
    "            total=len(future_to_key),\n",
    "            desc=f\"Writing {group.name}\"\n",
    "        ):\n",
    "            key = future_to_key[future]\n",
    "            try:\n",
    "                future.result()  # This will raise any exceptions\n",
    "            except Exception as exc:\n",
    "                print(f'Array {key} generated an exception: {exc}')\n",
    "                raise\n",
    "\n",
    "# %%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a0c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "src_group = zgroup.create_group(\"src_cell_data\", overwrite=True)\n",
    "tgt_group = zgroup.create_group(\"tgt_cell_data\", overwrite=True)\n",
    "\n",
    "\n",
    "# Use the fast threaded approach you already implemented\n",
    "write_cell_data_threaded(src_group, src_cell_data, chunk_size, shard_size, max_workers=14)\n",
    "print(\"done writing src_cell_data\")\n",
    "write_cell_data_threaded(tgt_group, tgt_cell_data, chunk_size, shard_size, max_workers=14)\n",
    "print(\"done writing tgt_cell_data\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d6fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "print(\"Writing mapping data\")\n",
    "mapping_data = zgroup.create_group(\"mapping_data\", overwrite=True)\n",
    "\n",
    "write_sharded(\n",
    "    mapping_data,\n",
    "    train_data_dict,\n",
    "    chunk_size=chunk_size,\n",
    "    shard_size=shard_size,\n",
    "    compressors=None,\n",
    ")\n",
    "print(\"done\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94414bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/functools.py:912: ImplicitModificationWarning: Transforming to str index.\n",
      "  return dispatch(args[0].__class__)(*args, **kw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 926.66 ms\n",
      "[########################################] | 100% Completed | 23.50 s\n",
      "[########################################] | 100% Completed | 262.74 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing source to cell data idcs: 100%|██████████| 50/50 [00:00<00:00, 60.94it/s]\n",
      "Computing target to cell data idcs: 100%|██████████| 56827/56827 [01:05<00:00, 864.75it/s]\n",
      "Computing source to cell data: 100%|██████████| 50/50 [00:00<00:00, 8124.09it/s]\n",
      "Computing target to cell data: 100%|██████████| 56827/56827 [00:06<00:00, 9053.27it/s] \n"
     ]
    }
   ],
   "source": [
    "import anndata as ad\n",
    "import h5py\n",
    "import zarr\n",
    "from cellflow.data._utils import write_sharded\n",
    "from anndata.experimental import read_lazy\n",
    "from cellflow.data import DataManager\n",
    "import cupy as cp\n",
    "import tqdm\n",
    "import dask\n",
    "import numpy as np\n",
    "\n",
    "print(\"loading data\")\n",
    "with h5py.File(\"/lustre/groups/ml01/workspace/100mil/100m_int_indices.h5ad\", \"r\") as f:\n",
    "    adata_all = ad.AnnData(\n",
    "        obs=ad.io.read_elem(f[\"obs\"]),\n",
    "        var=read_lazy(f[\"var\"]),\n",
    "        uns = read_lazy(f[\"uns\"]),\n",
    "        obsm = read_lazy(f[\"obsm\"]),\n",
    "    )\n",
    "\n",
    "dm = DataManager(adata_all,  \n",
    "    sample_rep=\"X_pca\",\n",
    "    control_key=\"control\",\n",
    "    perturbation_covariates={\"drugs\": (\"drug\",), \"dosage\": (\"dosage\",)},\n",
    "    perturbation_covariate_reps={\"drugs\": \"drug_embeddings\"},\n",
    "    sample_covariates=[\"cell_line\"],\n",
    "    sample_covariate_reps={\"cell_line\": \"cell_line_embeddings\"},\n",
    "    split_covariates=[\"cell_line\"],\n",
    "    max_combination_length=None,\n",
    "    null_value=0.0\n",
    ")\n",
    "\n",
    "cond_data = dm._get_condition_data(adata=adata_all)\n",
    "cell_data = dm._get_cell_data(adata_all)\n",
    "\n",
    "\n",
    "\n",
    "n_source_dists = len(cond_data.split_idx_to_covariates)\n",
    "n_target_dists = len(cond_data.perturbation_idx_to_covariates)\n",
    "\n",
    "tgt_cell_data = {}\n",
    "src_cell_data = {}\n",
    "gpu_per_cov_mask = cp.asarray(cond_data.perturbation_covariates_mask)\n",
    "gpu_spl_cov_mask = cp.asarray(cond_data.split_covariates_mask)\n",
    "\n",
    "for src_idx in tqdm.tqdm(range(n_source_dists), desc=\"Computing source to cell data idcs\"):\n",
    "    mask = gpu_spl_cov_mask == src_idx\n",
    "    src_cell_data[str(src_idx)] = {\n",
    "        \"cell_data_index\": cp.where(mask)[0].get(),\n",
    "    }\n",
    "\n",
    "for tgt_idx in tqdm.tqdm(range(n_target_dists), desc=\"Computing target to cell data idcs\"):\n",
    "    mask = gpu_per_cov_mask == tgt_idx\n",
    "    tgt_cell_data[str(tgt_idx)] = {\n",
    "        \"cell_data_index\": cp.where(mask)[0].get(),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "for src_idx in tqdm.tqdm(range(n_source_dists), desc=\"Computing source to cell data\"):\n",
    "    indices = src_cell_data[str(src_idx)][\"cell_data_index\"]\n",
    "    delayed_obj = dask.delayed(lambda x: cell_data[x])(indices)\n",
    "    src_cell_data[str(src_idx)][\"cell_data\"] = dask.array.from_delayed(delayed_obj, shape=(len(indices), cell_data.shape[1]), dtype=cell_data.dtype)\n",
    "\n",
    "for tgt_idx in tqdm.tqdm(range(n_target_dists), desc=\"Computing target to cell data\"):\n",
    "    indices = tgt_cell_data[str(tgt_idx)][\"cell_data_index\"]\n",
    "    delayed_obj = dask.delayed(lambda x: cell_data[x])(indices)\n",
    "    tgt_cell_data[str(tgt_idx)][\"cell_data\"] = dask.array.from_delayed(delayed_obj, shape=(len(indices), cell_data.shape[1]), dtype=cell_data.dtype)\n",
    "\n",
    "\n",
    "split_covariates_mask = np.asarray(cond_data.split_covariates_mask)\n",
    "perturbation_covariates_mask = np.asarray(cond_data.perturbation_covariates_mask)\n",
    "condition_data = {str(k): np.asarray(v) for k, v in (cond_data.condition_data or {}).items()}\n",
    "control_to_perturbation = {str(k): np.asarray(v) for k, v in (cond_data.control_to_perturbation or {}).items()}\n",
    "split_idx_to_covariates = {str(k): np.asarray(v) for k, v in (cond_data.split_idx_to_covariates or {}).items()}\n",
    "perturbation_idx_to_covariates = {\n",
    "    str(k): np.asarray(v) for k, v in (cond_data.perturbation_idx_to_covariates or {}).items()\n",
    "}\n",
    "perturbation_idx_to_id = {str(k): v for k, v in (cond_data.perturbation_idx_to_id or {}).items()}\n",
    "\n",
    "train_data_dict = {\n",
    "    \"split_covariates_mask\": split_covariates_mask,\n",
    "    \"perturbation_covariates_mask\": perturbation_covariates_mask,\n",
    "    \"split_idx_to_covariates\": split_idx_to_covariates,\n",
    "    \"perturbation_idx_to_covariates\": perturbation_idx_to_covariates,\n",
    "    \"perturbation_idx_to_id\": perturbation_idx_to_id,\n",
    "    \"condition_data\": condition_data,\n",
    "    \"control_to_perturbation\": control_to_perturbation,\n",
    "    \"max_combination_length\": int(cond_data.max_combination_length),\n",
    "    # \"src_cell_data\": src_cell_data,\n",
    "    # \"tgt_cell_data\": tgt_cell_data,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f224240",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/lustre/groups/ml01/workspace/100mil/tahoe.zarr\"\n",
    "zgroup = zarr.open_group(path, mode=\"w\")\n",
    "chunk_size = 65536\n",
    "shard_size = chunk_size * 16\n",
    "\n",
    "ad.settings.zarr_write_format = 3  # Needed to support sharding in Zarr\n",
    "\n",
    "def get_size(shape: tuple[int, ...], chunk_size: int, shard_size: int) -> tuple[int, int]:\n",
    "    shard_size_used = shard_size\n",
    "    chunk_size_used = chunk_size\n",
    "    if chunk_size > shape[0] or shard_size > shape[0]:\n",
    "        chunk_size_used = shard_size_used = shape[0]\n",
    "    return chunk_size_used, shard_size_used\n",
    "\n",
    "import dask.array as da\n",
    "from dask.diagnostics import ProgressBar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8aedd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60135, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_cell_data[str(0)][\"cell_data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "710434e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing src cell data:   0%|          | 0/50 [00:00<?, ?it/s]/ictstr01/home/icb/selman.ozleyen/.local/share/mamba/envs/lpert/lib/python3.12/site-packages/zarr/core/group.py:2566: ZarrUserWarning: The `compressor` argument is deprecated. Use `compressors` instead.\n",
      "  compressors = _parse_deprecated_compressor(\n",
      "Writing src cell data: 100%|██████████| 50/50 [00:00<00:00, 291.39it/s]\n",
      "Writing tgt cell data: 100%|██████████| 56827/56827 [04:35<00:00, 206.40it/s]\n"
     ]
    }
   ],
   "source": [
    "src_group = zgroup.create_group(\"src_cell_data\", overwrite=True)\n",
    "tgt_group = zgroup.create_group(\"tgt_cell_data\", overwrite=True)\n",
    "def write_arr(z_arr, arr, k):\n",
    "        z_arr[:] = arr\n",
    "        return k\n",
    "\n",
    "delayed_objs = []\n",
    "for k in tqdm.tqdm(src_cell_data.keys(), desc=\"Writing src cell data\"):\n",
    "    chunk_size_used, shard_size_used = get_size(src_cell_data[k][\"cell_data\"].shape, chunk_size, shard_size)\n",
    "    arr = src_cell_data[k][\"cell_data\"]\n",
    "\n",
    "    z_arr = src_group.create_array(\n",
    "        name=k,\n",
    "        shape=arr.shape,\n",
    "        chunks=(chunk_size_used, arr.shape[1]),\n",
    "        shards=(shard_size_used, arr.shape[1]),\n",
    "        compressors=None,\n",
    "        dtype=arr.dtype,\n",
    "    )\n",
    "    def write_arr(z_arr, arr):\n",
    "        z_arr[:] = arr\n",
    "        return k\n",
    "    \n",
    "    delayed_objs.append(dask.delayed(write_arr)(z_arr, arr, k))\n",
    "\n",
    "for k in tqdm.tqdm(tgt_cell_data.keys(), desc=\"Writing tgt cell data\"):\n",
    "    chunk_size_used, shard_size_used = get_size(tgt_cell_data[k][\"cell_data\"].shape, chunk_size, shard_size)\n",
    "    arr = tgt_cell_data[k][\"cell_data\"]\n",
    "    z_arr = tgt_group.create_array(\n",
    "        name=k,\n",
    "        shape=arr.shape,\n",
    "        chunks=(chunk_size_used, arr.shape[1]),\n",
    "        shards=(shard_size_used, arr.shape[1]),\n",
    "        compressors=None,\n",
    "        dtype=arr.dtype,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    delayed_objs.append(dask.delayed(write_arr)(z_arr, arr, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41b2a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    dask.compute(delayed_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f54507",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mapping_data = zarr.create_group(zgroup, \"mapping_data\")\n",
    "\n",
    "write_sharded(\n",
    "    mapping_data,\n",
    "    train_data_dict,\n",
    "    chunk_size=chunk_size,\n",
    "    shard_size=shard_size,\n",
    "    compressors=None,\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed731bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellflow.data import TrainSamplerWithPool, ZarrTrainingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62955dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_memory_cost(\n",
    "    data: ZarrTrainingData,\n",
    "    src_idx: int,\n",
    "    include_condition_data: bool = True\n",
    ") -> dict[str, int | list | dict]:\n",
    "    \"\"\"Calculate memory cost in bytes for a given source index and its target distributions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        The training data.\n",
    "    src_idx\n",
    "        The source distribution index.\n",
    "    include_condition_data\n",
    "        Whether to include condition data in memory calculations.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary with memory statistics in bytes for the source and its targets.\n",
    "    \"\"\"\n",
    "    if src_idx not in data.control_to_perturbation:\n",
    "        raise ValueError(f\"Source index {src_idx} not found in control_to_perturbation mapping\")\n",
    "    \n",
    "    # Get target indices for this source\n",
    "    target_indices = data.control_to_perturbation[src_idx]\n",
    "    \n",
    "    # Calculate memory for source cells\n",
    "    source_mask = data.split_covariates_mask == src_idx\n",
    "    n_source_cells = np.sum(source_mask)\n",
    "    source_memory = n_source_cells * data.cell_data.shape[1] * data.cell_data.dtype.itemsize\n",
    "    \n",
    "    # Calculate memory for target cells\n",
    "    target_memories = {}\n",
    "    total_target_memory = 0\n",
    "    \n",
    "    for target_idx in target_indices:\n",
    "        target_mask = data.perturbation_covariates_mask == target_idx\n",
    "        n_target_cells = np.sum(target_mask)\n",
    "        target_memory = n_target_cells * data.cell_data.shape[1] * data.cell_data.dtype.itemsize\n",
    "        target_memories[f\"target_{target_idx}\"] = target_memory\n",
    "        total_target_memory += target_memory\n",
    "    \n",
    "    # Calculate condition data memory if available and requested\n",
    "    condition_memory = 0\n",
    "    condition_details = {}\n",
    "    if include_condition_data and data.condition_data is not None:\n",
    "        for cond_name, cond_array in data.condition_data.items():\n",
    "            # Condition data is indexed by target indices\n",
    "            relevant_condition_size = len(target_indices) * cond_array.shape[1] * cond_array.dtype.itemsize\n",
    "            condition_details[f\"condition_{cond_name}\"] = relevant_condition_size\n",
    "            condition_memory += relevant_condition_size\n",
    "    \n",
    "    # Calculate total memory\n",
    "    total_memory = source_memory + total_target_memory + condition_memory\n",
    "    \n",
    "    # Calculate average target memory\n",
    "    avg_target_memory = total_target_memory // len(target_indices) if target_indices.size > 0 else 0\n",
    "    \n",
    "    result = {\n",
    "        \"source_idx\": src_idx,\n",
    "        \"target_indices\": target_indices.tolist(),\n",
    "        \"source_memory\": source_memory,\n",
    "        \"source_cell_count\": int(n_source_cells),\n",
    "        \"total_target_memory\": total_target_memory,\n",
    "        \"avg_target_memory\": avg_target_memory,\n",
    "        \"condition_memory\": condition_memory,\n",
    "        \"total_memory\": total_memory,\n",
    "        \"target_details\": target_memories,\n",
    "    }\n",
    "    \n",
    "    if condition_details:\n",
    "        result[\"condition_details\"] = condition_details\n",
    "        \n",
    "    return result\n",
    "\n",
    "def format_memory_stats(memory_stats: dict, unit: str = \"auto\", summary: bool = False) -> str:\n",
    "    \"\"\"Format memory statistics into a human-readable string.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    memory_stats\n",
    "        Dictionary with memory statistics from calculate_memory_cost.\n",
    "    unit\n",
    "        Memory unit to use for display. Options: 'B', 'KB', 'MB', 'GB', 'auto'.\n",
    "        If 'auto', the most appropriate unit will be chosen automatically.\n",
    "    summary\n",
    "        If True, includes a summary with average, min, and max target memory statistics\n",
    "        and omits detailed per-target breakdown.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Human-readable string representation of memory statistics.\n",
    "    \"\"\"\n",
    "    def format_bytes(bytes_value, unit=\"auto\"):\n",
    "        if unit == \"auto\":\n",
    "            # Choose appropriate unit\n",
    "            for unit in [\"B\", \"KB\", \"MB\", \"GB\"]:\n",
    "                if bytes_value < 1024 or unit == \"GB\":\n",
    "                    break\n",
    "                bytes_value /= 1024\n",
    "        elif unit == \"KB\":\n",
    "            bytes_value /= 1024\n",
    "        elif unit == \"MB\":\n",
    "            bytes_value /= (1024 * 1024)\n",
    "        elif unit == \"GB\":\n",
    "            bytes_value /= (1024 * 1024 * 1024)\n",
    "        \n",
    "        return f\"{bytes_value:.2f} {unit}\"\n",
    "    \n",
    "    src_idx = memory_stats[\"source_idx\"]\n",
    "    target_indices = memory_stats[\"target_indices\"]\n",
    "    \n",
    "    # Base information\n",
    "    lines = [\n",
    "        f\"Memory statistics for source index {src_idx} with {len(target_indices)} targets:\",\n",
    "        f\"- Source cells: {memory_stats['source_cell_count']} cells, {format_bytes(memory_stats['source_memory'], unit)}\",\n",
    "        f\"- Total memory: {format_bytes(memory_stats['total_memory'], unit)}\",\n",
    "    ]\n",
    "    \n",
    "    # Calculate min and max target memory if summary is requested\n",
    "    if summary and memory_stats[\"target_details\"]:\n",
    "        target_memories = list(memory_stats[\"target_details\"].values())\n",
    "        min_target = min(target_memories)\n",
    "        max_target = max(target_memories)\n",
    "        \n",
    "        lines.extend([\n",
    "            \"\\nTarget memory summary:\",\n",
    "            f\"- Total: {format_bytes(memory_stats['total_target_memory'], unit)}\",\n",
    "            f\"- Average: {format_bytes(memory_stats['avg_target_memory'], unit)}\",\n",
    "            f\"- Min: {format_bytes(min_target, unit)}\",\n",
    "            f\"- Max: {format_bytes(max_target, unit)}\",\n",
    "            f\"- Range: {format_bytes(max_target - min_target, unit)}\"\n",
    "        ])\n",
    "        \n",
    "        # Add condition memory summary if available\n",
    "        if memory_stats[\"condition_memory\"] > 0:\n",
    "            lines.append(f\"\\nCondition memory: {format_bytes(memory_stats['condition_memory'], unit)}\")\n",
    "    else:\n",
    "        # Detailed output (original format)\n",
    "        lines.extend([\n",
    "            f\"- Target memory: {format_bytes(memory_stats['total_target_memory'], unit)} total, {format_bytes(memory_stats['avg_target_memory'], unit)} average per target\",\n",
    "            f\"- Condition memory: {format_bytes(memory_stats['condition_memory'], unit)}\",\n",
    "            \"\\nTarget details:\"\n",
    "        ])\n",
    "        \n",
    "        for target_key, target_memory in memory_stats[\"target_details\"].items():\n",
    "            target_id = target_key.split(\"_\")[1]\n",
    "            lines.append(f\"  - Target {target_id}: {format_bytes(target_memory, unit)}\")\n",
    "        \n",
    "        if \"condition_details\" in memory_stats:\n",
    "            lines.append(\"\\nCondition details:\")\n",
    "            for cond_key, cond_memory in memory_stats[\"condition_details\"].items():\n",
    "                cond_name = cond_key.split(\"_\", 1)[1]\n",
    "                lines.append(f\"  - {cond_name}: {format_bytes(cond_memory, unit)}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "316e3a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ztd = ZarrTrainingData.read_zarr(data_paths[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d101216",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = calculate_memory_cost(ztd, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a79f9fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory statistics for source index 0 with 194 targets:\n",
      "- Source cells: 60135 cells, 68.82 MB\n",
      "- Total memory: 548.11 MB\n",
      "\n",
      "Target memory summary:\n",
      "- Total: 479.28 MB\n",
      "- Average: 2.47 MB\n",
      "- Min: 44.53 KB\n",
      "- Max: 6.35 MB\n",
      "- Range: 6.31 MB\n",
      "\n",
      "Condition memory: 4.55 KB\n"
     ]
    }
   ],
   "source": [
    "print(format_memory_stats(stats, summary=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c400080",
   "metadata": {},
   "outputs": [],
   "source": [
    "ztd_stats = {}\n",
    "for i in range(ztd.n_controls):\n",
    "    ztd_stats[i] = calculate_memory_cost(ztd, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "710fb69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_average_memory_per_source(stats_dict):\n",
    "    \"\"\"Print the average total memory per source index.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    stats_dict\n",
    "        Optional pre-calculated memory statistics dictionary.\n",
    "        If None, statistics will be calculated for all source indices.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Extract total memory for each source index\n",
    "    total_memories = [stats[\"total_memory\"] for stats in stats_dict.values()]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_memory = np.mean(total_memories)\n",
    "    min_memory = np.min(total_memories)\n",
    "    max_memory = np.max(total_memories)\n",
    "    median_memory = np.median(total_memories)\n",
    "    \n",
    "    # Format the output\n",
    "    def format_bytes(bytes_value):\n",
    "        for unit in [\"B\", \"KB\", \"MB\", \"GB\"]:\n",
    "            if bytes_value < 1024 or unit == \"GB\":\n",
    "                break\n",
    "            bytes_value /= 1024\n",
    "        return f\"{bytes_value:.2f} {unit}\"\n",
    "    \n",
    "    print(f\"Memory statistics across {len(stats_dict)} source indices:\")\n",
    "    print(f\"- Average total memory per source: {format_bytes(avg_memory)}\")\n",
    "    print(f\"- Minimum total memory: {format_bytes(min_memory)}\")\n",
    "    print(f\"- Maximum total memory: {format_bytes(max_memory)}\")\n",
    "    print(f\"- Median total memory: {format_bytes(median_memory)}\")\n",
    "    print(f\"- Range: {format_bytes(max_memory - min_memory)}\")\n",
    "    \n",
    "    # Identify source indices with min and max memory\n",
    "    min_idx = min(stats_dict.keys(), key=lambda k: stats_dict[k][\"total_memory\"])\n",
    "    max_idx = max(stats_dict.keys(), key=lambda k: stats_dict[k][\"total_memory\"])\n",
    "    \n",
    "    print(f\"\\nSource index with minimum memory: {min_idx} ({format_bytes(min_memory)})\")\n",
    "    print(f\"Source index with maximum memory: {max_idx} ({format_bytes(max_memory)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e2f8f809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory statistics across 50 source indices:\n",
      "- Average total memory per source: 423.18 MB\n",
      "- Minimum total memory: 4.33 MB\n",
      "- Maximum total memory: 1.29 GB\n",
      "- Median total memory: 404.51 MB\n",
      "- Range: 1.28 GB\n",
      "\n",
      "Source index with minimum memory: 39 (4.33 MB)\n",
      "Source index with maximum memory: 22 (1.29 GB)\n"
     ]
    }
   ],
   "source": [
    "print_average_memory_per_source(ztd_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91207483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellflow.data import TrainSamplerWithPool\n",
    "import numpy as np\n",
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17f1fc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing target to cell data idcs: 100%|██████████| 9980/9980 [00:11<00:00, 891.95it/s] \n",
      "Computing source to cell data idcs: 100%|██████████| 50/50 [00:00<00:00, 1232.06it/s]\n"
     ]
    }
   ],
   "source": [
    "tswp = TrainSamplerWithPool(ztd, batch_size=1024, pool_size=20, replacement_prob=0.01)\n",
    "tswp.init_pool_n_cache(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782380b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81017ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaced 47 with 34\n",
      "replaced 32 with 30\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "iter_times = []\n",
    "rng = np.random.default_rng(0)\n",
    "start_time = time.time()\n",
    "for iter in range(40):\n",
    "    batch = tswp.sample(rng)\n",
    "    end_time = time.time()\n",
    "    iter_times.append(end_time - start_time)\n",
    "    start_time = end_time\n",
    "\n",
    "print(\"average time per iteration: \", np.mean(iter_times))\n",
    "print(\"iterations per second: \", 1 / np.mean(iter_times))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe14be13",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "001e842a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pool_size': 20,\n",
       " 'avg_usage': 1.95,\n",
       " 'unique_sources': 20,\n",
       " 'pool_elements': array([31, 18, 47, 34, 12, 35, 29, 23, 32, 14,  6, 41, 25,  3,  1, 49, 24,\n",
       "        10, 46, 33]),\n",
       " 'usage_counts': array([2, 2, 3, 2, 1, 0, 2, 2, 2, 0, 3, 1, 2, 0, 3, 3, 2, 6, 1, 2])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tswp.get_pool_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07c55d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
